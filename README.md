# Emotion-XAI: Explainable AI for Social Media Emotion Detection

<div align="center">

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)
[![Gradio App](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/petlaz/emotion-xai)
[![GitHub](https://img.shields.io/badge/GitHub-Repository-black)](https://github.com/Petlaz/emotion_xai_project_clean)

**A production-ready explainable AI system for multi-label emotion detection in social media text**

[ğŸš€ **Live Demo**](https://huggingface.co/spaces/petlaz/emotion-xai) | [ğŸ“– **Documentation**](./docs/) | [ğŸ”¬ **Research Notebooks**](./notebooks/) | [â­ **Star on GitHub**](https://github.com/Petlaz/emotion_xai_project_clean)

</div>

---

## ğŸ¯ **Executive Summary**

This project delivers a **complete machine learning pipeline** for detecting and explaining emotions in social media text. Built with **state-of-the-art transformer models** and **explainable AI techniques**, it provides interpretable insights into human emotional expression in digital communications.

**ğŸ† Key Achievements:**
- **Production Model**: Fine-tuned DistilRoBERTa achieving **19.6% F1-macro** (1.2x baseline improvement)
- **Explainable AI**: Integrated SHAP and LIME for model interpretability
- **Interactive Interface**: Live web application with **real-time emotion analysis**
- **Scalable Architecture**: Production-ready deployment on Hugging Face Spaces

## ğŸš€ **Technology Stack & Features**

<table>
<tr>
<td width="50%">

### ğŸ”§ **Core Technologies**
- **Deep Learning**: PyTorch, Transformers
- **Model Architecture**: DistilRoBERTa (82M parameters)
- **Explainable AI**: SHAP, LIME
- **Clustering**: UMAP, HDBSCAN
- **Web Interface**: Gradio, Plotly
- **Deployment**: Hugging Face Spaces

</td>
<td width="50%">

### ğŸ“Š **Performance Metrics**
- **Dataset**: GoEmotions (211K samples, 28 emotions)
- **F1-Macro**: 0.196 (19.6% accuracy)
- **Baseline Improvement**: 1.2x performance gain
- **Processing Speed**: <1s per prediction
- **Model Size**: 82M parameters (optimized)

</td>
</tr>
</table>

### ğŸ¯ **Core Capabilities**

| Feature | Description | Status |
|---------|-------------|--------|
| **Multi-label Classification** | Detect 28 different emotions simultaneously | âœ… Production Ready |
| **Explainable Predictions** | SHAP/LIME explanations for model transparency | âœ… Fully Integrated |
| **Real-time Analysis** | Interactive web interface with instant results | âœ… Live Demo Available |
| **Batch Processing** | Analyze multiple texts efficiently | âœ… Optimized Pipeline |
| **Theme Discovery** | Unsupervised clustering for emotion patterns | âœ… Advanced Analytics |
| **Production Deployment** | Scalable cloud-based serving | âœ… HF Spaces Deployed |

### ğŸ“ˆ **Business Value**

- **Social Media Monitoring**: Automated emotion analysis for brand sentiment
- **Content Moderation**: Detect emotional tone for platform safety
- **Market Research**: Understand customer emotional responses
- **Mental Health**: Monitor emotional patterns in digital communications

## Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/Petlaz/emotion_xai_project_clean.git
cd emotion_xai_project

# Install dependencies
pip install -r requirements.txt

# Optional: Install in development mode
pip install -e .
```

### Basic Usage

```python
from emotion_xai.utils.device import resolve_device
from emotion_xai.data.preprocessing import load_dataset, prepare_features
from emotion_xai.models.baseline import BaselineModel
from emotion_xai.explainability.explanations import explain_with_shap

# Setup device optimization (automatic detection: CUDA/MPS/CPU)
device = resolve_device()

# Load GoEmotions dataset (211,225 samples with 28 emotion labels)
data = load_dataset("data/raw/goemotions.csv")
features = prepare_features(data, "text")

# Train baseline model (TF-IDF + Logistic Regression)
model = BaselineModel()
model.fit(train_texts, train_labels)

# Evaluate model performance (Current: F1-macro 0.161)
metrics = model.evaluate(val_texts, val_labels)
print(f"F1-macro: {metrics['f1_macro']:.3f}")

# Generate explanations
explanations = explain_with_shap(model, sample_texts)
```

## ğŸ”¥ Production Transformer Training

### Quick Training Options

```bash
# Option 1: Using the runner script (recommended)
./run_production_training.sh test    # Quick test (5K samples, ~5-10 min)
./run_production_training.sh full    # Full training (147K samples, ~30-60 min)

# Option 2: Direct Python execution  
python scripts/train_transformer_production.py --config configs/test_training.json
python scripts/train_transformer_production.py --config configs/production_training.json

# Option 3: Resume from checkpoint
python scripts/train_transformer_production.py \
  --config configs/production_training.json \
  --resume models/distilroberta_production_*/checkpoint-1500
```

### Current Best Model
- **âœ… Production Model**: `models/distilroberta_production_20251130_044054/`
- **ğŸ† Training Complete**: 6,500/11,540 steps (56% of 5 epochs completed)
- **ğŸ¯ Performance**: F1-macro 19.6%, F1-micro 30.4%, Hamming Acc 96.2%
- **ğŸ“ˆ Achievement**: 87% loss reduction (0.695 â†’ 0.089), 1.2x baseline improvement

## ğŸ—ï¸ **System Architecture**

```
emotion_xai_project/
â”œâ”€â”€ ğŸ“Š data/                     # Dataset and processed features
â”‚   â”œâ”€â”€ raw/                     # Original GoEmotions data
â”‚   â””â”€â”€ processed/               # Cleaned and split datasets
â”œâ”€â”€ ğŸ“” notebooks/                # Jupyter analysis notebooks (clean structure)
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb    # âœ… EDA and data quality analysis
â”‚   â”œâ”€â”€ 02_modeling.ipynb           # âœ… Baseline model development
â”‚   â”œâ”€â”€ 03_finetuning.ipynb         # âœ… Transformer model training
â”‚   â”œâ”€â”€ 04_explainability.ipynb     # âœ… Production XAI analysis
â”‚   â””â”€â”€ 05_clustering_analysis.ipynb # âœ… Clustering & theme discovery
â”œâ”€â”€ ğŸ¤– models/                   # Trained model artifacts
â”‚   â”œâ”€â”€ distilroberta_production_20251130_044054/  # âœ… Best production model
â”‚   â”œâ”€â”€ saved_models/            # Baseline model artifacts
â”‚   â””â”€â”€ cluster_embeddings/      # âœ… Clustering pipeline & embeddings cache
â”œâ”€â”€ ğŸ”§ scripts/                  # Production scripts and utilities
â”‚   â”œâ”€â”€ train_transformer_production.py  # Main training script
â”‚   â”œâ”€â”€ use_trained_model.py     # Model inference utilities
â”‚   â”œâ”€â”€ download_goemotions.py   # Dataset download utility
â”‚   â””â”€â”€ test_*.py               # Various testing scripts
â”œâ”€â”€ âš™ï¸  config/                   # General application configurations (YAML)
â”‚   â”œâ”€â”€ production.yaml          # Production deployment settings
â”‚   â”œâ”€â”€ development.yaml         # Development environment config
â”‚   â”œâ”€â”€ default.yaml            # Default configuration settings
â”‚   â””â”€â”€ mac_optimizations.yaml   # Mac M1/M2 specific optimizations
â”œâ”€â”€ ï¿½ results/                  # Training results and visualizations
â”‚   â”œâ”€â”€ metrics/                 # Performance metrics and statistics
â”‚   â”œâ”€â”€ plots/                   # All generated visualizations
â”‚   â””â”€â”€ clustering_analysis_*.json # âœ… Clustering analysis results
â”œâ”€â”€ ğŸ“¦ emotion_xai/              # Core library package
â”‚   â”œâ”€â”€ data/                    # âœ… Data processing utilities
â”‚   â”œâ”€â”€ models/                  # âœ… Model implementations
â”‚   â”œâ”€â”€ explainability/         # âœ… XAI explanations (SHAP/LIME)
â”‚   â”œâ”€â”€ clustering/             # âœ… Theme discovery & clustering pipeline
â”‚   â”œâ”€â”€ utils/                  # âœ… Utility functions and helpers
â”‚   â””â”€â”€ cli.py                  # Command-line interface
â”œâ”€â”€ ğŸŒ app/                      # âœ… Web interface (Complete)
â”‚   â”œâ”€â”€ gradio_app.py           # Full-featured Gradio application (434 lines)
â”‚   â””â”€â”€ __init__.py            # Package initialization
â”œâ”€â”€ ğŸš€ app.py                   # âœ… HF Spaces entry point (production ready)
â”œâ”€â”€ ğŸ“‹ requirements_gradio.txt   # âœ… Gradio deployment dependencies  
â”œâ”€â”€ ğŸ“š README_HF_SPACES.md      # âœ… Hugging Face Spaces documentation
â”œâ”€â”€ ğŸ³ docker/                   # Containerization
â”‚   â”œâ”€â”€ Dockerfile              # Production container setup
â”‚   â””â”€â”€ requirements.txt        # Docker-specific dependencies
â”œâ”€â”€ ğŸ“š docs/                     # Documentation (organized)
â”‚   â”œâ”€â”€ README.md               # Documentation overview
â”‚   â”œâ”€â”€ project_plan.md         # Complete project plan
â”‚   â”œâ”€â”€ documentation_report.md # Comprehensive completion report
â”‚   â”œâ”€â”€ development.md          # Development setup guide
â”‚   â”œâ”€â”€ getting_started.md      # Getting started guide
â”‚   â””â”€â”€ mac_optimization.md     # Mac M1/M2 optimization guide
â”œâ”€â”€ ğŸ§ª tests/                    # Test suite
â”‚   â”œâ”€â”€ conftest.py             # Test configuration
â”‚   â”œâ”€â”€ fixtures/               # Test fixtures
â”‚   â”œâ”€â”€ unit/                   # Unit tests
â”‚   â””â”€â”€ integration/            # Integration tests
â”œâ”€â”€ ğŸ“„ logs/                     # Application logs directory
â””â”€â”€ ğŸ”§ Configuration Files       # Project configuration
    â”œâ”€â”€ requirements.txt         # Main Python dependencies
    â”œâ”€â”€ pyproject.toml          # Package configuration
    â”œâ”€â”€ setup.cfg               # Setup tools configuration
    â”œâ”€â”€ MANIFEST.in             # Package manifest
    â”œâ”€â”€ LICENSE                 # Project license
    â”œâ”€â”€ CHANGELOG.md            # Version history
    â”œâ”€â”€ CONTRIBUTING.md         # Contribution guidelines
    â”œâ”€â”€ .gitignore              # Git ignore patterns
    â””â”€â”€ run_production_training.sh # Production training script
```

## ğŸ” Model Usage

### Using Trained Models

```python
# Load the trained model for inference
from scripts.use_trained_model import EmotionPredictor

# Initialize predictor with best model
predictor = EmotionPredictor("models/distilroberta_production_20251130_044054")

# Single prediction
emotions = predictor.predict("I love this product but the delivery was slow")
print(emotions)  # {'joy': 0.85, 'disappointment': 0.73, ...}

# Batch prediction
results = predictor.predict_batch([
    "Amazing customer service!",
    "The product broke after one day",
    "Decent quality for the price"
])

# Interactive demo
predictor.run_interactive_demo()  # Launches Gradio interface
```

### Web Interface

Launch the interactive Gradio interface:

```bash
python app/gradio_app.py
# OR
python app.py  # HF Spaces entry point
```

Access at `http://localhost:7860` for:
- Real-time emotion prediction with 4-decimal precision
- Interactive Plotly visualizations
- Model explanations with SHAP/LIME
- Batch processing capabilities
- Professional UI with instant launch examples

### ğŸš€ Hugging Face Spaces Deployment

Ready for one-click deployment to Hugging Face Spaces:

1. **Files Ready**:
   - âœ… `app.py` - Main entry point
   - âœ… `requirements_gradio.txt` - Dependencies
   - âœ… `README_HF_SPACES.md` - Spaces documentation

2. **Deploy Command**:
```bash
# From your HF Spaces repository
git add .
git commit -m "Deploy Emotion-XAI app"
git push
```

3. **Features**:
   - âœ… Instant launch (<30s)
   - âœ… Pre-loaded examples
   - âœ… Public sharing capability
   - âœ… Production DistilRoBERTa model (82M params)
   - âœ… 4-decimal precision scores

## ğŸ§  Explainable AI

### SHAP Explanations

```python
from emotion_xai.explainability.explanations import explain_with_shap

# Generate SHAP explanations for predictions
explanations = explain_with_shap(
    model=predictor.model,
    tokenizer=predictor.tokenizer, 
    text="The service was excellent but expensive",
    top_k_emotions=5
)

# Visualize feature importance
explanations.plot()    # Shows word-level contributions
```

### LIME Explanations

```python
from emotion_xai.explainability.explanations import explain_with_lime

# Generate LIME explanations
lime_exp = explain_with_lime(
    predictor=predictor,
    text="Fast shipping, great quality product!",
    num_features=10
)

lime_exp.show_in_notebook()  # Interactive visualization
```

## ğŸ“ˆ Performance Monitoring

### Training Progress

Monitor training with built-in logging:

```python
# Check latest training metrics
from pathlib import Path
import json

# Load final training results
results_path = Path("results/production_training/production_results_20251130_074958.json")
with open(results_path) as f:
    results = json.load(f)
    
print(f"Final F1-macro: {results['test_results']['f1_macro']:.4f}")
print(f"Training duration: {results['training_info']['duration_minutes']:.1f} minutes")
print(f"Model location: {results['model_path']}")
```

## ğŸ› ï¸ Development

### Setting Up Development Environment

1. Clone the repository:
```bash
git clone https://github.com/Petlaz/emotion-xai-project_clean.git
cd emotion-xai-project
```

2. Create virtual environment:
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

3. Install in development mode:
```bash
pip install -e ".[dev]"
```

### Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=emotion_xai --cov-report=html

# Run specific test types
pytest tests/unit/          # Unit tests only
pytest tests/integration/   # Integration tests only
```

### Code Quality

```bash
# Code formatting
black emotion_xai/ scripts/ tests/
isort emotion_xai/ scripts/ tests/

# Linting  
flake8 emotion_xai/ scripts/ tests/
mypy emotion_xai/

# Security check
bandit -r emotion_xai/
```

## CLI Usage

The package provides a command-line interface for common tasks:

```bash
# Test device optimizations (automatic CUDA/MPS/CPU detection)
python emotion_xai/utils/device.py

# Train baseline model with optimized settings
emotion-xai train-baseline --data-path data/raw/goemotions.csv --text-column text

# Train transformer model with automatic device detection
emotion-xai train-transformer --model-name distilroberta-base --epochs 3 --batch-size 16

# Download GoEmotions dataset
python scripts/download_goemotions.py
```

## ğŸš€ Deployment

### Hugging Face Spaces (Recommended)

**âœ… Ready for one-click deployment!**

**Pre-deployment Checklist:**
- âœ… `app.py` - HF Spaces entry point 
- âœ… `requirements_gradio.txt` - Complete dependencies
- âœ… `README_HF_SPACES.md` - Spaces metadata and documentation
- âœ… Gradio interface with instant launch capability
- âœ… Production model included (82M parameters)
- âœ… 4-decimal precision for emotion scores
- âœ… Public sharing enabled with professional UI

**Deploy Steps:**
1. Create new Space on Hugging Face
2. Upload project files
3. Set Space to use `app.py` as main file
4. App launches automatically with <30s startup time

### Docker Support

```bash
# Build the Docker image
docker build -t emotion-xai .

# Run the container
docker run -p 7860:7860 emotion-xai

# With GPU support
docker run --gpus all -p 7860:7860 emotion-xai
```

### Production Features

The system is designed for production with:
- **âœ… Scalable inference** with batch processing
- **âœ… Public API** via Gradio sharing links
- **âœ… Model versioning** with checkpoint management
- **âœ… Real-time monitoring** with comprehensive metrics
- **âœ… Professional UI** with instant examples and explanations
- **âœ… Cross-platform** compatibility (CUDA/MPS/CPU auto-detection)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes with tests
4. Run quality checks (`black`, `flake8`, `pytest`)
5. Commit changes (`git commit -m 'Add amazing feature'`)
6. Push to branch (`git push origin feature/amazing-feature`)
7. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Acknowledgments

- **GoEmotions Dataset**: Google Research for the comprehensive emotion dataset
- **Hugging Face**: Transformers library and model hub
- **SHAP/LIME**: Explainable AI framework contributions
- **Gradio**: Interactive ML interface framework

## ğŸ“ Support

For questions, issues, or contributions:
- **GitHub Issues**: [Create an issue](https://github.com/Petlaz/emotion_xai_project_clean/issues)
- **Documentation**: Check the `docs/` directory for detailed guides
- **Examples**: See `notebooks/` for usage examples

---

---

## ğŸš€ **Live Demo & Links**

<div align="center">

### ğŸŒŸ **Try the Live Application**
[![Hugging Face Spaces](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/petlaz/emotion-xai)

**[ğŸ­ Launch Emotion-XAI App](https://huggingface.co/spaces/petlaz/emotion-xai)**

### ğŸ“Š **Project Resources**
| Resource | Link | Description |
|----------|------|-------------|
| ğŸš€ **Live Demo** | [HF Spaces](https://huggingface.co/spaces/petlaz/emotion-xai) | Interactive web application |
| ğŸ“‚ **Source Code** | [GitHub Repository](https://github.com/Petlaz/emotion_xai_project_clean) | Complete codebase |
| ğŸ“– **Documentation** | [Technical Docs](./docs/) | Comprehensive guides |
| ğŸ”¬ **Research** | [Jupyter Notebooks](./notebooks/) | Analysis workflows |

</div>

## ğŸ† **Project Status**

<div align="center">

### âœ… **PRODUCTION READY**
**Complete 6-phase ML pipeline successfully deployed**

ğŸ¯ **F1-Macro: 19.6%** | ğŸ¤– **DistilRoBERTa: 82M params** | ğŸ“Š **GoEmotions: 211K samples** | ğŸŒ **Live on HF Spaces**

*Built with modern MLOps practices, explainable AI, and production-grade deployment*

</div>