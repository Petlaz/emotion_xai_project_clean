# Emotion-Aware Customer Feedback Analysis with Explainable AI

[![CI/CD Pipeline](https://github.com/Petlaz/emotion-xai-project/workflows/CI%2FCD%20Pipeline/badge.svg)](https://github.com/Petlaz/emotion-xai-project/actions)
[![codecov](https://codecov.io/gh/Petlaz/emotion-xai-project/branch/main/graph/badge.svg)](https://codecov.io/gh/Petlaz/emotion-xai-project)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)

A comprehensive package for analyzing customer feedback using transformer models, explainable AI techniques, and clustering for theme discovery.

## Features

- **Multi-label emotion classification** using fine-tuned transformer models (DistilRoBERTa)
- **Explainable AI** with SHAP and LIME explanations for model interpretability
- **Theme discovery** through clustering analysis using UMAP + HDBSCAN
- **Interactive web interface** built with Gradio for real-time analysis
- **Comprehensive evaluation** metrics and visualizations
- **Production-ready** with Docker support and CI/CD pipelines

## Quick Start

### Installation

```bash
pip install emotion-xai
```

### Basic Usage

```python
from emotion_xai.data.preprocessing import load_dataset, prepare_features
from emotion_xai.models.baseline import train_baseline
from emotion_xai.explainability.explanations import explain_with_shap

# Load and prepare data (data files are in project root data/ directory)
data = load_dataset("data/raw/goemotions.csv")  # Raw data location
features = prepare_features(data, "text_column")

# Train baseline model
model = train_baseline(features, labels)

# Generate explanations
explanations = explain_with_shap(model, sample_texts)
```

## Project Structure

```
emotion_xai_project/
â”œâ”€â”€ emotion_xai/                 # ðŸ“¦ Main Python package (installable)
â”‚   â”œâ”€â”€ __init__.py             #    Package initialization & exports
â”‚   â”œâ”€â”€ cli.py                  #    Command-line interface entry point
â”‚   â”œâ”€â”€ data/                   #    ðŸ“ Data processing modules (Python code)
â”‚   â”‚   â”œâ”€â”€ __init__.py         #    NOT actual data - just processing code!
â”‚   â”‚   â””â”€â”€ preprocessing.py    #    Text cleaning & feature preparation functions
â”‚   â”œâ”€â”€ models/                 #    Machine learning models
â”‚   â”‚   â”œâ”€â”€ __init__.py         
â”‚   â”‚   â”œâ”€â”€ baseline.py         #    TF-IDF + Logistic Regression baseline
â”‚   â”‚   â””â”€â”€ transformer.py     #    DistilRoBERTa fine-tuning utilities
â”‚   â”œâ”€â”€ explainability/         #    Model interpretation & XAI
â”‚   â”‚   â”œâ”€â”€ __init__.py         
â”‚   â”‚   â””â”€â”€ explanations.py    #    SHAP & LIME explanation generators
â”‚   â”œâ”€â”€ clustering/             #    Theme discovery & clustering
â”‚   â”‚   â”œâ”€â”€ __init__.py         
â”‚   â”‚   â””â”€â”€ feedback_clustering.py  # UMAP + HDBSCAN clustering
â”‚   â””â”€â”€ utils/                  #    Shared utilities & configuration
â”‚       â”œâ”€â”€ __init__.py         
â”‚       â””â”€â”€ config.py           #    Configuration management classes
â”‚
â”œâ”€â”€ tests/                      # ðŸ§ª Test suite (pytest-based)
â”‚   â”œâ”€â”€ conftest.py             #    Shared test fixtures & configuration
â”‚   â”œâ”€â”€ unit/                   #    Unit tests for individual modules
â”‚   â”‚   â”œâ”€â”€ test_preprocessing.py
â”‚   â”‚   â””â”€â”€ test_baseline.py
â”‚   â”œâ”€â”€ integration/            #    Integration & end-to-end tests
â”‚   â”‚   â””â”€â”€ test_pipeline.py
â”‚   â””â”€â”€ fixtures/               #    Test data & mock objects
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ docs/                       # ðŸ“š Documentation (Markdown & Sphinx)
â”‚   â”œâ”€â”€ README.md               #    Documentation overview
â”‚   â”œâ”€â”€ getting_started.md      #    Installation & quick start guide
â”‚   â”œâ”€â”€ development.md          #    Development setup & contributing
â”‚   â””â”€â”€ documentation_report.md #    Project documentation report
â”‚
â”œâ”€â”€ config/                     # âš™ï¸ Configuration files (YAML-based)
â”‚   â”œâ”€â”€ default.yaml            #    Default configuration settings
â”‚   â”œâ”€â”€ development.yaml        #    Development environment config
â”‚   â””â”€â”€ production.yaml         #    Production environment config
â”‚
â”œâ”€â”€ data/                       # ðŸ’¾ ACTUAL data files (at project root)
â”‚   â”œâ”€â”€ raw/                    #    ðŸ“ Original datasets (e.g., goemotions.csv)
â”‚   â”‚   â””â”€â”€ .gitkeep            #    (data files themselves are gitignored)
â”‚   â””â”€â”€ processed/              #    ðŸ“ Cleaned & preprocessed data files  
â”‚       â””â”€â”€ .gitkeep            #    (processed data files are gitignored)
â”‚
â”œâ”€â”€ models/                     # ðŸ¤– Saved model artifacts & checkpoints
â”‚   â”œâ”€â”€ distilroberta_finetuned/ #    Fine-tuned transformer models
â”‚   â”‚   â””â”€â”€ .gitkeep            #    (model files gitignored)
â”‚   â””â”€â”€ cluster_embeddings/     #    Clustering model artifacts
â”‚       â””â”€â”€ .gitkeep            #    (model files gitignored)
â”‚
â”œâ”€â”€ notebooks/                  # ðŸ““ Jupyter notebooks for exploration
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb     # EDA & data understanding
â”‚   â”œâ”€â”€ 02_finetuning.ipynb          # Model training experiments
â”‚   â”œâ”€â”€ 03_explainability.ipynb      # XAI analysis & visualization
â”‚   â””â”€â”€ 04_clustering_analysis.ipynb # Theme discovery analysis
â”‚
â”œâ”€â”€ app/                        # ðŸŒ Web application (Gradio interface)
â”‚   â””â”€â”€ gradio_app.py           #    Interactive demo & API server
â”‚
â”œâ”€â”€ docker/                     # ðŸ³ Containerization & deployment
â”‚   â”œâ”€â”€ Dockerfile              #    Multi-stage Docker build
â”‚   â””â”€â”€ requirements.txt        #    Docker-specific dependencies
â”‚
â”œâ”€â”€ scripts/                    # ðŸ› ï¸ Utility scripts for setup & automation
â”‚   â””â”€â”€ download_goemotions.py  #    GoEmotions dataset download utility
â”‚
â”œâ”€â”€ logs/                       # ðŸ“ Application logs (gitignored content)
â”‚   â””â”€â”€ .gitignore              #    Log files exclusion rules
â”‚
â”œâ”€â”€ .github/workflows/          # ðŸš€ CI/CD automation (GitHub Actions)
â”‚   â””â”€â”€ ci.yml                  #    Test, lint, & deployment pipeline
â”‚
â”œâ”€â”€ pyproject.toml              # ðŸ“‹ Modern Python packaging configuration
â”œâ”€â”€ requirements.txt            # ðŸ“¦ Production dependencies
â”œâ”€â”€ requirements-dev.txt        # ðŸ› ï¸ Development dependencies
â”œâ”€â”€ setup.cfg                   # âš™ï¸ Tool configuration (pytest, flake8, mypy)
â”œâ”€â”€ .pre-commit-config.yaml     # ðŸ” Pre-commit hooks for code quality
â”œâ”€â”€ MANIFEST.in                 # ðŸ“„ Package distribution files
â”œâ”€â”€ CHANGELOG.md                # ðŸ“… Version history & release notes
â””â”€â”€ CONTRIBUTING.md             # ðŸ¤ Contribution guidelines & workflow
```

**Key Design Principles:**
- **Separation of Concerns**: Each directory has a single, clear responsibility
- **Scalability**: Structure supports growing from prototype to production  
- **Reproducibility**: Configuration management & environment isolation
- **Collaboration**: Clear testing, documentation & contribution workflows

> **ðŸš¨ IMPORTANT - Data vs Code Separation:**
> - `emotion_xai/data/` = **Python modules** for data processing (code)
> - `data/` (project root) = **Actual dataset files** (CSV, JSON, etc.)
> - This follows best practices: code is installable, data stays with project

> **Note**: This structure follows Python packaging best practices with:
> - **Package code** in `emotion_xai/` (installable via pip)
> - **Project data** in `data/` (separate from package code)
> - **Configuration** externalized in `config/` 
> - **Tests** isolated in `tests/` with proper fixtures

## Development

### Setting Up Development Environment

1. Clone the repository:
```bash
git clone https://github.com/Petlaz/emotion-xai-project.git
cd emotion-xai-project
```

2. Create virtual environment:
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

3. Install in development mode:
```bash
pip install -e ".[dev]"
```

4. Install pre-commit hooks:
```bash
pre-commit install
```

### Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=emotion_xai --cov-report=html

# Run specific test types
pytest tests/unit/          # Unit tests only
pytest tests/integration/   # Integration tests only
```

### Code Quality

```bash
# Format code
black emotion_xai tests

# Sort imports
isort emotion_xai tests

# Lint code
flake8 emotion_xai tests

# Type checking
mypy emotion_xai
```

## CLI Usage

The package provides a command-line interface for common tasks:

```bash
# Train baseline model (using data from project root data/ directory)
emotion-xai train-baseline --data-path data/raw/goemotions.csv --text-column text

# Train transformer model with custom parameters
emotion-xai train-transformer --model-name distilroberta-base --epochs 3 --batch-size 16

# Show help for all available commands
emotion-xai --help
```

## Documentation

- [Getting Started](docs/getting_started.md)
- [User Guide](docs/user_guide.md)
- [Development Guide](docs/development.md)
- [API Reference](docs/api_reference.md)

## Contributing

Please read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct and the process for submitting pull requests.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- Built with [Transformers](https://huggingface.co/transformers/) by Hugging Face
- Uses [SHAP](https://github.com/slundberg/shap) for explainable AI
- Clustering powered by [UMAP](https://umap-learn.readthedocs.io/) and [HDBSCAN](https://hdbscan.readthedocs.io/)
- Web interface built with [Gradio](https://gradio.app/)